{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CqrVkkJ1jRvC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj-EiRqz0V4e"
      },
      "source": [
        "# import 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBo_Hyw3_ENJ"
      },
      "source": [
        "한글 폰트 받기\n",
        "코드 실행 후 런타임 재시작하여 다시 실행"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt -qq -y install fonts-nanum > /dev/null # 폰트 나눔을 구글코랩에 설치하고 경로를 파악함\n",
        "import matplotlib.font_manager as fm \n",
        "fontpath='/user/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font=fm.FontProperties(fname=fontpath,size=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw6Os2Vh9BXM",
        "outputId": "a0b2a18a-c12a-4a3a-9505-381b73986b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev0OR1Te_KMe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore') \n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3DGHund0tAR"
      },
      "source": [
        "STT - Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds9AED9_v4ut",
        "outputId": "b23d6a61-8eca-48a6-b606-e758bad8930a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-tpbfttxx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-tpbfttxx\n",
            "  Resolved https://github.com/openai/whisper.git to commit 248b6cb124225dd263bb9bd32d060b6517e067f8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230314)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.27.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798075 sha256=f5a6b93aacfc837496d2efab233c50d0e118d333e4be8c96678573be030a647c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_tdflanb/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230314 tiktoken-0.3.3\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,346 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,202 kB]\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,051 kB]\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,581 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,218 kB]\n",
            "Fetched 9,755 kB in 3s (3,031 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "34 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZUFXiNv4uy"
      },
      "outputs": [],
      "source": [
        "#!whisper 'audio1.wav' --model medium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx4VHJngwKBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe101229-5df5-4a9c-b570-f1f961c6b29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:11<00:00, 138MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3MsGia0xRG"
      },
      "source": [
        "# Gaze Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgmY115EfZVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d30a0e-a7a2-437e-d76d-8542194311b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GazeTracking'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 113 (delta 34), reused 30 (delta 30), pack-reused 67\u001b[K\n",
            "Receiving objects: 100% (113/113), 69.00 MiB | 31.63 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/antoinelame/GazeTracking.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrZjDWfgrsdH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class Pupil(object):\n",
        "    \"\"\"\n",
        "    This class detects the iris of an eye and estimates\n",
        "    the position of the pupil\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eye_frame, threshold):\n",
        "        self.iris_frame = None\n",
        "        self.threshold = threshold\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "        self.detect_iris(eye_frame)\n",
        "\n",
        "    @staticmethod\n",
        "    def image_processing(eye_frame, threshold):\n",
        "        \"\"\"Performs operations on the eye frame to isolate the iris\n",
        "        Arguments:\n",
        "            eye_frame (numpy.ndarray): Frame containing an eye and nothing else\n",
        "            threshold (int): Threshold value used to binarize the eye frame\n",
        "        Returns:\n",
        "            A frame with a single element representing the iris\n",
        "        \"\"\"\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        new_frame = cv2.bilateralFilter(eye_frame, 10, 15, 15)\n",
        "        new_frame = cv2.erode(new_frame, kernel, iterations=3)\n",
        "        new_frame = cv2.threshold(new_frame, threshold, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "        return new_frame\n",
        "\n",
        "    def detect_iris(self, eye_frame):\n",
        "        \"\"\"Detects the iris and estimates the position of the iris by\n",
        "        calculating the centroid.\n",
        "        Arguments:\n",
        "            eye_frame (numpy.ndarray): Frame containing an eye and nothing else\n",
        "        \"\"\"\n",
        "        self.iris_frame = self.image_processing(eye_frame, self.threshold)\n",
        "\n",
        "        contours, _ = cv2.findContours(self.iris_frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[-2:]\n",
        "        contours = sorted(contours, key=cv2.contourArea)\n",
        "\n",
        "        try:\n",
        "            moments = cv2.moments(contours[-2])\n",
        "            self.x = int(moments['m10'] / moments['m00'])\n",
        "            self.y = int(moments['m01'] / moments['m00'])\n",
        "        except (IndexError, ZeroDivisionError):\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdbeb5qPrYti"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import cv2\n",
        "#from .pupil import Pupil\n",
        "\n",
        "\n",
        "class Calibration(object):\n",
        "    \"\"\"\n",
        "    This class calibrates the pupil detection algorithm by finding the\n",
        "    best binarization threshold value for the person and the webcam.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nb_frames = 20\n",
        "        self.thresholds_left = []\n",
        "        self.thresholds_right = []\n",
        "\n",
        "    def is_complete(self):\n",
        "        \"\"\"Returns true if the calibration is completed\"\"\"\n",
        "        return len(self.thresholds_left) >= self.nb_frames and len(self.thresholds_right) >= self.nb_frames\n",
        "\n",
        "    def threshold(self, side):\n",
        "        \"\"\"Returns the threshold value for the given eye.\n",
        "        Argument:\n",
        "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
        "        \"\"\"\n",
        "        if side == 0:\n",
        "            return int(sum(self.thresholds_left) / len(self.thresholds_left))\n",
        "        elif side == 1:\n",
        "            return int(sum(self.thresholds_right) / len(self.thresholds_right))\n",
        "\n",
        "    @staticmethod\n",
        "    def iris_size(frame):\n",
        "        \"\"\"Returns the percentage of space that the iris takes up on\n",
        "        the surface of the eye.\n",
        "        Argument:\n",
        "            frame (numpy.ndarray): Binarized iris frame\n",
        "        \"\"\"\n",
        "        frame = frame[5:-5, 5:-5]\n",
        "        height, width = frame.shape[:2]\n",
        "        nb_pixels = height * width\n",
        "        nb_blacks = nb_pixels - cv2.countNonZero(frame)\n",
        "        return nb_blacks / nb_pixels\n",
        "\n",
        "    @staticmethod\n",
        "    def find_best_threshold(eye_frame):\n",
        "        \"\"\"Calculates the optimal threshold to binarize the\n",
        "        frame for the given eye.\n",
        "        Argument:\n",
        "            eye_frame (numpy.ndarray): Frame of the eye to be analyzed\n",
        "        \"\"\"\n",
        "        average_iris_size = 0.48\n",
        "        trials = {}\n",
        "\n",
        "        for threshold in range(5, 100, 5):\n",
        "            iris_frame = Pupil.image_processing(eye_frame, threshold)\n",
        "            trials[threshold] = Calibration.iris_size(iris_frame)\n",
        "\n",
        "        best_threshold, iris_size = min(trials.items(), key=(lambda p: abs(p[1] - average_iris_size)))\n",
        "        return best_threshold\n",
        "\n",
        "    def evaluate(self, eye_frame, side):\n",
        "        \"\"\"Improves calibration by taking into consideration the\n",
        "        given image.\n",
        "        Arguments:\n",
        "            eye_frame (numpy.ndarray): Frame of the eye\n",
        "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
        "        \"\"\"\n",
        "        threshold = self.find_best_threshold(eye_frame)\n",
        "\n",
        "        if side == 0:\n",
        "            self.thresholds_left.append(threshold)\n",
        "        elif side == 1:\n",
        "            self.thresholds_right.append(threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdTgfZoFreCs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "#from .pupil import Pupil\n",
        "\n",
        "\n",
        "class Eye(object):\n",
        "    \"\"\"\n",
        "    This class creates a new frame to isolate the eye and\n",
        "    initiates the pupil detection.\n",
        "    \"\"\"\n",
        "\n",
        "    LEFT_EYE_POINTS = [36, 37, 38, 39, 40, 41]\n",
        "    RIGHT_EYE_POINTS = [42, 43, 44, 45, 46, 47]\n",
        "\n",
        "    def __init__(self, original_frame, landmarks, side, calibration):\n",
        "        self.frame = None\n",
        "        self.origin = None\n",
        "        self.center = None\n",
        "        self.pupil = None\n",
        "        self.landmark_points = None\n",
        "\n",
        "        self._analyze(original_frame, landmarks, side, calibration)\n",
        "\n",
        "    @staticmethod\n",
        "    def _middle_point(p1, p2):\n",
        "        \"\"\"Returns the middle point (x,y) between two points\n",
        "\n",
        "        Arguments:\n",
        "            p1 (dlib.point): First point\n",
        "            p2 (dlib.point): Second point\n",
        "        \"\"\"\n",
        "        x = int((p1.x + p2.x) / 2)\n",
        "        y = int((p1.y + p2.y) / 2)\n",
        "        return (x, y)\n",
        "\n",
        "    def _isolate(self, frame, landmarks, points):\n",
        "        \"\"\"Isolate an eye, to have a frame without other part of the face.\n",
        "\n",
        "        Arguments:\n",
        "            frame (numpy.ndarray): Frame containing the face\n",
        "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
        "            points (list): Points of an eye (from the 68 Multi-PIE landmarks)\n",
        "        \"\"\"\n",
        "        region = np.array([(landmarks.part(point).x, landmarks.part(point).y) for point in points])\n",
        "        region = region.astype(np.int32)\n",
        "        self.landmark_points = region\n",
        "\n",
        "        # Applying a mask to get only the eye\n",
        "        height, width = frame.shape[:2]\n",
        "        black_frame = np.zeros((height, width), np.uint8)\n",
        "        mask = np.full((height, width), 255, np.uint8)\n",
        "        cv2.fillPoly(mask, [region], (0, 0, 0))\n",
        "        eye = cv2.bitwise_not(black_frame, frame.copy(), mask=mask)\n",
        "\n",
        "        # Cropping on the eye\n",
        "        margin = 5\n",
        "        min_x = np.min(region[:, 0]) - margin\n",
        "        max_x = np.max(region[:, 0]) + margin\n",
        "        min_y = np.min(region[:, 1]) - margin\n",
        "        max_y = np.max(region[:, 1]) + margin\n",
        "\n",
        "        self.frame = eye[min_y:max_y, min_x:max_x]\n",
        "        self.origin = (min_x, min_y)\n",
        "\n",
        "        height, width = self.frame.shape[:2]\n",
        "        self.center = (width / 2, height / 2)\n",
        "\n",
        "    def _blinking_ratio(self, landmarks, points):\n",
        "        \"\"\"Calculates a ratio that can indicate whether an eye is closed or not.\n",
        "        It's the division of the width of the eye, by its height.\n",
        "\n",
        "        Arguments:\n",
        "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
        "            points (list): Points of an eye (from the 68 Multi-PIE landmarks)\n",
        "\n",
        "        Returns:\n",
        "            The computed ratio\n",
        "        \"\"\"\n",
        "        left = (landmarks.part(points[0]).x, landmarks.part(points[0]).y)\n",
        "        right = (landmarks.part(points[3]).x, landmarks.part(points[3]).y)\n",
        "        top = self._middle_point(landmarks.part(points[1]), landmarks.part(points[2]))\n",
        "        bottom = self._middle_point(landmarks.part(points[5]), landmarks.part(points[4]))\n",
        "\n",
        "        eye_width = math.hypot((left[0] - right[0]), (left[1] - right[1]))\n",
        "        eye_height = math.hypot((top[0] - bottom[0]), (top[1] - bottom[1]))\n",
        "\n",
        "        try:\n",
        "            ratio = eye_width / eye_height\n",
        "        except ZeroDivisionError:\n",
        "            ratio = None\n",
        "\n",
        "        return ratio\n",
        "\n",
        "    def _analyze(self, original_frame, landmarks, side, calibration):\n",
        "        \"\"\"Detects and isolates the eye in a new frame, sends data to the calibration\n",
        "        and initializes Pupil object.\n",
        "\n",
        "        Arguments:\n",
        "            original_frame (numpy.ndarray): Frame passed by the user\n",
        "            landmarks (dlib.full_object_detection): Facial landmarks for the face region\n",
        "            side: Indicates whether it's the left eye (0) or the right eye (1)\n",
        "            calibration (calibration.Calibration): Manages the binarization threshold value\n",
        "        \"\"\"\n",
        "        if side == 0:\n",
        "            points = self.LEFT_EYE_POINTS\n",
        "        elif side == 1:\n",
        "            points = self.RIGHT_EYE_POINTS\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        self.blinking = self._blinking_ratio(landmarks, points)\n",
        "        self._isolate(original_frame, landmarks, points)\n",
        "\n",
        "        if not calibration.is_complete():\n",
        "            calibration.evaluate(self.frame, side)\n",
        "\n",
        "        threshold = calibration.threshold(side)\n",
        "        self.pupil = Pupil(self.frame, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spmz7y8WroaI"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "#from .eye import Eye\n",
        "#from .calibration import Calibration\n",
        "\n",
        "\n",
        "class GazeTracking(object):\n",
        "    \"\"\"\n",
        "    This class tracks the user's gaze.\n",
        "    It provides useful information like the position of the eyes\n",
        "    and pupils and allows to know if the eyes are open or closed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.frame = None\n",
        "        self.eye_left = None\n",
        "        self.eye_right = None\n",
        "        self.calibration = Calibration()\n",
        "\n",
        "        # _face_detector is used to detect faces\n",
        "        self._face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "        # _predictor is used to get facial landmarks of a given face\n",
        "        #cwd = os.path.abspath(os.path.dirname(__file__))\n",
        "        #model_path = os.path.abspath(os.path.join(cwd, \"trained_models/shape_predictor_68_face_landmarks.dat\"))\n",
        "        #self._predictor = dlib.shape_predictor(model_path)\n",
        "        self._predictor = dlib.shape_predictor('/content/GazeTracking/gaze_tracking/trained_models/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "    @property\n",
        "    def pupils_located(self):\n",
        "        \"\"\"Check that the pupils have been located\"\"\"\n",
        "        try:\n",
        "            int(self.eye_left.pupil.x)\n",
        "            int(self.eye_left.pupil.y)\n",
        "            int(self.eye_right.pupil.x)\n",
        "            int(self.eye_right.pupil.y)\n",
        "            return True\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _analyze(self):\n",
        "        \"\"\"Detects the face and initialize Eye objects\"\"\"\n",
        "        frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = self._face_detector(frame)\n",
        "\n",
        "        try:\n",
        "            landmarks = self._predictor(frame, faces[0])\n",
        "            self.eye_left = Eye(frame, landmarks, 0, self.calibration)\n",
        "            self.eye_right = Eye(frame, landmarks, 1, self.calibration)\n",
        "\n",
        "        except IndexError:\n",
        "            self.eye_left = None\n",
        "            self.eye_right = None\n",
        "\n",
        "    def refresh(self, frame):\n",
        "        \"\"\"Refreshes the frame and analyzes it.\n",
        "        Arguments:\n",
        "            frame (numpy.ndarray): The frame to analyze\n",
        "        \"\"\"\n",
        "        self.frame = frame\n",
        "        self._analyze()\n",
        "\n",
        "    def pupil_left_coords(self):\n",
        "        \"\"\"Returns the coordinates of the left pupil\"\"\"\n",
        "        if self.pupils_located:\n",
        "            x = self.eye_left.origin[0] + self.eye_left.pupil.x\n",
        "            y = self.eye_left.origin[1] + self.eye_left.pupil.y\n",
        "            return (x, y)\n",
        "\n",
        "    def pupil_right_coords(self):\n",
        "        \"\"\"Returns the coordinates of the right pupil\"\"\"\n",
        "        if self.pupils_located:\n",
        "            x = self.eye_right.origin[0] + self.eye_right.pupil.x\n",
        "            y = self.eye_right.origin[1] + self.eye_right.pupil.y\n",
        "            return (x, y)\n",
        "\n",
        "    def horizontal_ratio(self):\n",
        "        \"\"\"Returns a number between 0.0 and 1.0 that indicates the\n",
        "        horizontal direction of the gaze. The extreme right is 0.0,\n",
        "        the center is 0.5 and the extreme left is 1.0\n",
        "        \"\"\"\n",
        "        if self.pupils_located:\n",
        "            pupil_left = self.eye_left.pupil.x / (self.eye_left.center[0] * 2 - 10)\n",
        "            pupil_right = self.eye_right.pupil.x / (self.eye_right.center[0] * 2 - 10)\n",
        "            return (pupil_left + pupil_right) / 2\n",
        "\n",
        "    def vertical_ratio(self):\n",
        "        \"\"\"Returns a number between 0.0 and 1.0 that indicates the\n",
        "        vertical direction of the gaze. The extreme top is 0.0,\n",
        "        the center is 0.5 and the extreme bottom is 1.0\n",
        "        \"\"\"\n",
        "        if self.pupils_located:\n",
        "            pupil_left = self.eye_left.pupil.y / (self.eye_left.center[1] * 2 - 10)\n",
        "            pupil_right = self.eye_right.pupil.y / (self.eye_right.center[1] * 2 - 10)\n",
        "            return (pupil_left + pupil_right) / 2\n",
        "\n",
        "    def is_right(self):\n",
        "        \"\"\"Returns true if the user is looking to the right\"\"\"\n",
        "        if self.pupils_located:\n",
        "            return self.horizontal_ratio() <= 0.35\n",
        "\n",
        "    def is_left(self):\n",
        "        \"\"\"Returns true if the user is looking to the left\"\"\"\n",
        "        if self.pupils_located:\n",
        "            return self.horizontal_ratio() >= 0.65\n",
        "\n",
        "    def is_center(self):\n",
        "        \"\"\"Returns true if the user is looking to the center\"\"\"\n",
        "        if self.pupils_located:\n",
        "            return self.is_right() is not True and self.is_left() is not True\n",
        "\n",
        "    def is_blinking(self):\n",
        "        \"\"\"Returns true if the user closes his eyes\"\"\"\n",
        "        if self.pupils_located:\n",
        "            blinking_ratio = (self.eye_left.blinking + self.eye_right.blinking) / 2\n",
        "            return blinking_ratio > 3.8\n",
        "\n",
        "    def annotated_frame(self):\n",
        "        \"\"\"Returns the main frame with pupils highlighted\"\"\"\n",
        "        frame = self.frame.copy()\n",
        "\n",
        "        if self.pupils_located:\n",
        "            color = (0, 255, 0)\n",
        "            x_left, y_left = self.pupil_left_coords()\n",
        "            x_right, y_right = self.pupil_right_coords()\n",
        "            cv2.line(frame, (x_left - 5, y_left), (x_left + 5, y_left), color)\n",
        "            cv2.line(frame, (x_left, y_left - 5), (x_left, y_left + 5), color)\n",
        "            cv2.line(frame, (x_right - 5, y_right), (x_right + 5, y_right), color)\n",
        "            cv2.line(frame, (x_right, y_right - 5), (x_right, y_right + 5), color)\n",
        "\n",
        "        return frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3822KUI-a63U"
      },
      "outputs": [],
      "source": [
        "gaze = GazeTracking()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YFiXmmr06xf"
      },
      "source": [
        "Konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbLjk00qihoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b709e58-70e6-49ac-83e8-38a5cf671156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz9CwCB6iVk1"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "from konlpy.utils import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVyrOm5b0_sw"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tArSTfPT0YRE"
      },
      "source": [
        "# 최종 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgLx36VN4QM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97028a2-1b1b-4486-9757-5f9c860b3628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=926f3b9f13109a16643b0b6926d01202b32ba1b046911ca4bbb926c4f54566c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YQdQtpJvXA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd12b0ea-41a8-4ccc-a314-df625925ca5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2255-lSz57U"
      },
      "outputs": [],
      "source": [
        "video_path = ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAXN8C5q6XcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49191ebd-e644-458c-b289-e4ce8b06a39b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "video_path # mp4 file\n",
        "wav_path = \"/content\"             # 저장할 path\n",
        "save_file_id = \"/content/ .wav\" # 저장할 file name \n",
        "\n",
        "command = \"ffmpeg -i {} -ab 160k -ac 2 -ar 44100 -vn {}\".format(video_path, os.path.join(wav_path, save_file_id))    \n",
        "subprocess.call(command, shell=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lMMaaOTzahQ"
      },
      "outputs": [],
      "source": [
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 표현이 사용된 횟수 출력\n",
        "\n",
        "def count_words(word_list):\n",
        "    word_count = {}\n",
        "    for word in word_list:\n",
        "        if word in word_count:\n",
        "            word_count[word] += 1\n",
        "        else:\n",
        "            word_count[word] = 1\n",
        "    return word_count"
      ],
      "metadata": {
        "id": "qlepi90c65Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "konRN7-u__57"
      },
      "outputs": [],
      "source": [
        "# 애매한 표현, 말더듬기 출력\n",
        "\n",
        "def speech_analyze(text):\n",
        "  unclear = ['그냥', '아마', '아마도', '대충', '좀', '약간']\n",
        "  umm = ['어', '음', '아', '그']\n",
        "  unclear_list = []\n",
        "  umm_list = []\n",
        "  unclear_num = 0\n",
        "  umm_num = 0\n",
        "\n",
        "  punctuation = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  # 문장부호 삭제\n",
        "  result = text.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "  words = result.split(\" \")\n",
        "  word_count = len(words)\n",
        "\n",
        "  for i in range(word_count):\n",
        "    for j in range(len(unclear)):\n",
        "      if words[i]==unclear[j]: \n",
        "        unclear_num+=1\n",
        "        unclear_list.append(words[i])\n",
        "    for k in range(len(umm)):\n",
        "      if words[i]==umm[k]:\n",
        "        umm_num+=1\n",
        "        umm_list.append(words[i])\n",
        "\n",
        "  print(unclear_num, unclear_list, unclear_num/word_count)\n",
        "  print(umm_num, umm_list, umm_num/word_count)\n",
        "\n",
        "  unclear_count = count_words(unclear_list)\n",
        "  umm_count = count_words(umm_list)\n",
        "\n",
        "  for word, count in unclear_count.items():\n",
        "    print(f'{word}: {count}')\n",
        "    \n",
        "  for word, count in umm_count.items():\n",
        "    print(f'{word}: {count}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXAIspn5338-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# 문장 단위 말하는 속도 그래프 출력\n",
        "\n",
        "def speech_speed(result):\n",
        "  remove_set = {' ', '.', ',', '?', '!' }\n",
        "  word_rate = []\n",
        "\n",
        "  for i in range(len(result['segments'])):\n",
        "    text_set = result['segments'][i]\n",
        "    start = result['segments'][i]['start']\n",
        "    end = result['segments'][i]['end']\n",
        "\n",
        "    text_list = list(text_set)\n",
        "    text_list = [j for j in text_list if j not in remove_set]\n",
        "\n",
        "    word_rate.append(len(text_list)/(end-start))\n",
        "\n",
        "\n",
        "  print(word_rate)\n",
        "\n",
        "  x = []\n",
        "  for i in range(len(result['segments'])):\n",
        "    x.append(result['segments'][i]['start'])\n",
        "  y = [j for j in word_rate]\n",
        "  print(np.nanmean(word_rate))\n",
        "\n",
        "  #plt.subplot(2, 1, 2)  \n",
        "  plt.axhline(y=np.nanmean(word_rate))\n",
        "  plt.plot(x, y) \n",
        "  plt.xlabel(\"x\") \n",
        "  plt.ylabel(\"y\")\n",
        "\n",
        "  return word_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGmDIEUVPMDI"
      },
      "outputs": [],
      "source": [
        "# stt\n",
        "\n",
        "def speech_blink(result, video_path):\n",
        "  # stt 워드 클라우드\n",
        "  text = result[\"text\"]\n",
        "\n",
        "  okt = Okt()\n",
        "  nouns = okt.nouns(text) # 명사만 추출\n",
        "\n",
        "  words = [n for n in nouns if len(n) > 1] # 단어의 길이가 1개인 것은 제외\n",
        "\n",
        "  c = Counter(words) \n",
        "\n",
        "  wc = WordCloud(font_path='user/share/fonts/truetype/nanum/NanumBarunGothic.ttf', width=400, height=400, scale=2.0, max_font_size=250)\n",
        "  gen = wc.generate_from_frequencies(c)\n",
        "  plt.figure()\n",
        "  plt.imshow(gen)\n",
        "\n",
        "  # 음성 빠르기\n",
        "\n",
        "\n",
        "  # 텍스트에서 단어 수 카운트\n",
        "  words_2 = text.split()\n",
        "  word_count = len(words_2)\n",
        "\n",
        "  # 음성 파일 길이 계산\n",
        "  audio_file = save_file_id\n",
        "  duration = librosa.get_duration(filename=audio_file)\n",
        "\n",
        "  # spm 계산\n",
        "  spm = word_count / duration\n",
        "\n",
        "  # 결과 출력\n",
        "  print(\"Word Count: {}\".format(word_count))\n",
        "  print(\"Duration: {:.2f} seconds\".format(duration))\n",
        "  print(\"Speech Rate: {:.2f} words per second\".format(spm))\n",
        "  if spm>=2.0: print(\"말하는 속도가 너무 빠릅니다. 속도 그래프를 참고해 적절한 말하기 속도를 찾아보세요.\")\n",
        "\n",
        "  #  Gaze tracking\n",
        "\n",
        "  import cv2\n",
        "  #from gaze_tracking import GazeTracking\n",
        "\n",
        "  gaze = GazeTracking()\n",
        "  video = cv2.VideoCapture(video_path) # 비디오 파일 경로 입력\n",
        "  frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) # 비디오 프레임 수 확인\n",
        "\n",
        "  blink_count = 0\n",
        "  gaze_movement_count = 0\n",
        "\n",
        "  while True:\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    gaze.refresh(frame)\n",
        "    frame = gaze.annotated_frame()\n",
        "\n",
        "    if gaze.is_blinking():\n",
        "        blink_count += 1\n",
        "\n",
        "    if gaze.pupil_left_coords() and gaze.pupil_right_coords():\n",
        "        gaze_movement_count += 1\n",
        "\n",
        "  blink_rate = blink_count / frame_count # 눈 깜빡임 비율\n",
        "  gaze_movement_rate = gaze_movement_count / frame_count # 눈동자 움직임 비율\n",
        "\n",
        "  print(f\"Blink rate: {blink_rate}\")\n",
        "  if blink_rate>=0.13: print(\"눈을 너무 많이 깜빡입니다.\")\n",
        "  print(f\"Gaze movement rate: {gaze_movement_rate}\")\n",
        "\n",
        "  speech_analyze(text)\n",
        "\n",
        "\n",
        "  #word_rate = speech_speed(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(save_file_id)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "5Xa618wCIcQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech_blink(result)"
      ],
      "metadata": {
        "id": "iG9_seonIUFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech_speed(result)"
      ],
      "metadata": {
        "id": "RDiWrVecIZty"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}